数据挖掘
参考书（项目）：Machine-Learning-in-Action-Python3





# 支持向量机

支持向量机（Support Vector Machine, SVM）是一种强大的有监督机器学习算法，主要用于解决分类和回归问题。其基本思想是通过构建一个最大间隔的超平面（在二维空间即为直线，在高维空间则为超平面）来对训练数据进行最佳分割，从而实现对未知数据的预测分类。以下是支持向量机的核心特点和工作原理：

1. **最大间隔分离**：SVM旨在找到一个最优超平面，使得两类样本点被分隔得尽可能远。这里的“远”可以用间隔（margin）来衡量，即超平面与最近样本点之间的距离。最大间隔意味着超平面两侧的分类边界最宽，这样可以提高模型的泛化能力，使其对未见过的数据有更好的区分能力。

2. **支持向量**：在找到的最大间隔超平面上，与之距离最近的一些样本点被称为“支持向量”。这些点对超平面的位置起决定性作用，其他样本点即使发生微小变动也不会影响超平面。因此，SVM具有稀疏性，仅依赖少数支持向量进行决策，使得模型更为稳健。

3. **核函数**：对于线性不可分的数据（即无法用一条直线或超平面完全分开），SVM引入了核函数（Kernel Function）。核函数通过将原始输入空间映射到更高维的特征空间，使得在新空间中数据变得线性可分。常用的核函数包括线性核、多项式核、高斯核（RBF，Radial Basis Function）等。核函数的选择和参数调整对模型性能至关重要。

4. **软间隔与正则化**：在实际应用中，数据可能存在噪声或异常值，导致严格的最大间隔分离难以实现。为此，SVM引入了软间隔（Soft Margin）的概念，允许部分样本位于间隔边界之内，但需付出一定的惩罚代价。这通过在优化目标中加入正则化项（如L1或L2范数）来实现，平衡间隔最大化与模型复杂度之间的关系。

5. **分类与回归**：SVM最初主要用于分类任务，通过构建分离超平面实现两类数据的区分。对于多分类问题，通常采用“一对一”、“一对多”或“多类投票”等策略进行扩展。此外，SVM还可以通过修改损失函数和决策规则，应用于回归问题，如ε-不敏感SVM回归。

总结来说，支持向量机利用最大间隔分离原则，结合核函数技巧，能够在复杂、高维数据集上构建出具有强泛化能力的分类或回归模型。其主要优点包括：处理非线性问题能力强、泛化性能好、模型解释性强（通过支持向量）、能够处理高维数据且对大规模数据有一定的计算效率（由于只关注支持向量）。然而，SVM对参数选择敏感，且当数据集规模非常大时，核方法可能导致计算成本增加。




# 逻辑回归

逻辑回归（Logistic Regression）是一种广义线性模型，主要用于解决二分类问题，但经过适当扩展后也可用于多分类和评分任务。虽然其名称中包含“回归”，实际上逻辑回归是一种概率预测模型，用于估计给定一组特征条件下，某个事件发生的概率，并基于此概率进行类别决策。以下是逻辑回归的主要特点和工作原理：

1. **模型形式**：逻辑回归模型使用sigmoid函数（逻辑函数）将线性回归模型的连续输出转换为(0,1)之间的概率值。sigmoid函数的公式为：

   \[
   f(x) = \frac{1}{1 + e^{-x}}
   \]

   其中，\( x \) 是线性函数 \( z = w_0 + w_1x_1 + w_2x_2 + ... + w_px_p \) 的输出（即权重向量 \( w \) 与特征向量 \( x \) 的点积加上偏置项 \( w_0 \)）。sigmoid函数的输出可以理解为正类（如“购买”或“是”）的概率，而 \( 1 - f(x) \) 则为负类的概率。

2. **似然函数与最大似然估计**：逻辑回归通过最大化观测数据的对数似然函数来估计模型参数（权重向量 \( w \) 和偏置项 \( w_0 \)）。给定训练数据集 \( \{(x^{(i)}, y^{(i)})\}_{i=1}^N \)，其中 \( x^{(i)} \) 是第 \( i \) 个样本的特征向量，\( y^{(i)} \) 是对应的标签（0或1），对数似然函数定义为：

   \[
   L(w) = \sum_{i=1}^N y^{(i)}\log(f(x^{(i)})) + (1-y^{(i)})\log(1-f(x^{(i)}))
   \]

   最大化对数似然函数等价于最小化负对数似然损失（即交叉熵损失），这是逻辑回归模型的优化目标。

3. **梯度下降与牛顿法等优化算法**：为了求解最大似然估计中的参数 \( w \)，通常采用梯度下降、拟牛顿法（如BFGS、L-BFGS）或其他优化算法。通过迭代更新参数，使损失函数逐步降低直至收敛，得到最优模型参数。

4. **特征缩放与正则化**：逻辑回归对特征尺度敏感，通常需要进行特征缩放（如标准化或归一化）以保证各个特征对模型的影响大致相当。此外，为防止过拟合，逻辑回归模型常引入L1或L2正则化（岭回归或套索回归），在优化目标中添加正则化项以限制模型复杂度。

5. **多分类与softmax回归**：对于多分类问题，逻辑回归可通过构造多个二分类模型（一对一、一对多或多类softmax回归）进行扩展。softmax回归是逻辑回归在多分类任务上的直接推广，其输出层使用softmax函数将线性预测值转化为多类别的概率分布。

总结来说，逻辑回归是一种基于线性模型与sigmoid函数的二分类概率预测模型，通过最大化对数似然函数估计模型参数，适用于处理各种二分类问题。其优点包括模型解释性强、易于理解和实现、计算效率较高，适用于大规模数据。然而，逻辑回归假设特征间的线性关系以及输出概率与输入特征之间的线性关系，对于非线性关系较强的复杂数据集可能需要进行特征工程或结合其他非线性模型使用。此外，逻辑回归对异常值较为敏感，且在处理多分类问题时可能需要进行适当的扩展。


# 集成学习算法

集成学习（Ensemble Learning）是一种机器学习方法，它通过构建并结合多个模型（称为“个体学习器”或“基学习器”）的预测结果，以期获得比任何单一模型更好的预测性能。集成学习的基本思想是“三个臭皮匠，顶个诸葛亮”，即多个模型的集体智慧往往能优于单个模型。以下是一些常见的集成学习算法及其特点：

1. **Bagging（Bootstrap Aggregating）**：
   - **原理**：通过自助采样（Bootstrap Sampling）从原始数据集中抽取多个子集（每个子集大小与原始数据集相同，但允许重复抽样），在每个子集上独立训练一个基学习器。最终预测时，各基学习器的预测结果通过投票（分类任务）或平均（回归任务）等方式融合。
   - **典型算法**：随机森林（Random Forest）。在Bagging的基础上，引入决策树构建过程中的随机属性选择，进一步增加个体学习器的多样性。

2. **Boosting**：
   - **原理**：Boosting是一种序列化方法，每一轮迭代都根据前一轮的学习结果调整样本权重，重点关注被前一轮预测错误的样本。这样，后续的基学习器会更专注于解决前一轮学习器的不足之处。所有基学习器的预测结果按照其在集成中的重要性（权重）进行加权组合。
   - **典型算法**：
     - AdaBoost（Adaptive Boosting）：动态调整样本权重和基学习器权重。
     - Gradient Boosting：基于梯度提升框架，通过最小化损失函数的负梯度方向来生成新的基学习器。
     - XGBoost、LightGBM、CatBoost：高效实现的梯度提升树算法，针对大规模数据和并行计算进行了优化。

3. **Stacking**：
   - **原理**：Stacking（堆叠）分为两阶段。第一阶段，使用不同的基学习器在原始数据集上训练并做出预测。第二阶段，将第一阶段得到的预测结果作为新特征，与原始特征一起组成新的数据集，训练一个“元学习器”（通常是逻辑回归、SVM、神经网络等）来整合各基学习器的输出，作出最终预测。

4. **Blending**：
   - **原理**：与Stacking类似，但元学习器不在训练过程中参与反馈，而是仅在所有基学习器训练完成后，使用交叉验证得到的预测结果作为新特征，训练元学习器。这种方式避免了训练过程中的过拟合风险。

5. **投票（Voting）**：
   - **原理**：简单多数投票（硬投票）：直接依据多数基学习器的预测结果决定最终类别；加权投票（软投票）：根据基学习器的历史表现赋予不同权重，按权重进行投票。
   - **典型应用场景**：当拥有多种类型的基学习器（如决策树、SVM、神经网络等）时，可以通过投票方法融合其预测结果。

6. **平均（Averaging）**：
   - **原理**：回归任务中，直接取各基学习器预测结果的算术平均值作为集成模型的预测输出。这种方法在基学习器性能相近且独立时效果良好。

集成学习的主要优点包括：
- 提高模型精度和稳定性：通过结合多个模型，降低单个模型的误差，提高整体预测性能。
- 防止过拟合：个体学习器的多样性有助于减少模型对特定细节的过度依赖，增强模型的泛化能力。
- 利用不同模型的优势：集成不同类型的基学习器可以整合各自的优势，应对复杂数据分布。

需要注意的是，集成学习可能会增加模型的计算复杂性和训练时间，尤其是在基学习器数量较多或模型结构复杂的情况下。此外，选择合适的个体学习器、融合策略以及调参也是集成学习中需要考虑的关键问题。

16 8 4 1
4  3 2 0
7


-3 -4 -5 -7

0.0011101

